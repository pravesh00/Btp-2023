{
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing the dataset\nX = pd.read_csv('../Datasets/NB.csv')\nX['Fault'] = 0\n\ny = pd.read_csv('../Datasets/IR - 7.csv')\ny['Fault'] = 1\n\nX_train, X_test = train_test_split(X, test_size = 0.2, shuffle=False, random_state = 0)\ny_train, y_test = train_test_split(y, test_size = 0.2, shuffle=False, random_state = 0)\n\ntrain = X_train.append(y_train)\ntrain = train.reset_index(drop=True)\n\ntest = X_test.append(y_test)\ntest = test.reset_index(drop=True)\n\ntrain_data = train[['DE', 'FE', 'Fault']]\nn_cluster = range(1, 20)\nkmeans = [KMeans(n_clusters=i).fit(train_data) for i in n_cluster]\nscores = [kmeans[i].score(train_data) for i in range(len(kmeans))]\nfig, ax = plt.subplots(figsize=(10,6))\nax.plot(n_cluster, scores)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show();\n\nX = train[['DE', 'FE', 'Fault']]\nX = X.reset_index(drop=True)\nkm = KMeans(n_clusters=10)\nkm.fit(X)\nkm.predict(X)\nlabels = km.labels_\n#Plotting\nfig = plt.figure(1, figsize=(7,7))\nax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=30, azim=145)\nax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,2],\n          c=labels.astype(np.float), edgecolor=\"k\")\nax.set_xlabel(\"DE\")\nax.set_ylabel(\"FE\")\nax.set_zlabel(\"Fault\")\nplt.title(\"K Means\", fontsize=14);\n\nX = train_data.values\nX_std = StandardScaler().fit_transform(X)\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\neig_pairs.sort(key = lambda x: x[0], reverse= True)\ntot = sum(eig_vals)\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\ncum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n\nplt.figure(figsize=(10, 5))\nplt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')\nplt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.show();\n\ndef getDistanceByPoint(data, model):\n    distance = pd.Series()\n    for i in range(0,len(data)):\n        Xa = np.array(data.loc[i])\n        Xb = model.cluster_centers_[model.labels_[i]-1]\n        distance.at[i] = np.linalg.norm(Xa-Xb)\n    return distance\n\noutliers_fraction = 0.01\n# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\ndistance = getDistanceByPoint(train_data, kmeans[9])\nnumber_of_outliers = int(outliers_fraction*len(distance))\nthreshold = distance.nlargest(number_of_outliers).min()\n# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly) \ntrain['anomaly'] = (distance >= threshold).astype(int)\n\n# visualisation of anomaly with cluster view\nfig, ax = plt.subplots(figsize=(10,6))\ncolors = {0:'blue', 1:'red'}\nax.scatter(train['DE'], train['FE'], c=train[\"anomaly\"].apply(lambda x: colors[x]))\nplt.xlabel('principal feature1')\nplt.ylabel('principal feature2')\nplt.show();\n\ntrain_anomalies = train[train['anomaly'] == 1]\n\nf, (ax2) = plt.subplots(figsize=(18, 6))\nax2.scatter(train_anomalies.index, train_anomalies.DE, label='anomaly', color='red', s=10)\nax2.plot(train.index, train.DE, label='DE');\nplt.xlim((0,len(train.index)))\nplt.title('K means')\nplt.xlabel('Data points')\nplt.ylabel('Distance from centroid')\nplt.legend();\nplt.show();\ntest_data = test[['DE', 'FE', 'Fault']]\noutliers_fraction = 0.01\n\n# Get the distance between each point and its nearest centroid. The biggest distances are considered as anomalies.\ndistance = getDistanceByPoint(test_data, kmeans[9])\nnumber_of_outliers = int(outliers_fraction * len(distance))\nthreshold = distance.nlargest(number_of_outliers).min()\n\n# Anomaly1 contains the anomaly result of the above method cluster (0:normal, 1:anomaly).\ntest['anomaly'] = (distance >= threshold).astype(int)\n\n# Visualize the data with anomalies highlighted.\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = {0: 'blue', 1: 'red'}\nax.scatter(test['DE'], test['FE'], c=test[\"anomaly\"].apply(lambda x: colors[x]))\nplt.xlabel('Principal Feature 1')\nplt.ylabel('Principal Feature 2')\nplt.show()\n\n# Get the anomalies.\ntest_anomalies = test[test['anomaly'] == 1]\n\n# Visualize the anomalies.\nf, (ax1) = plt.subplots(figsize=(18, 6))\nax1.scatter(test_anomalies.index, test_anomalies.DE, label='Anomaly', color='red', s=10)\nax1.plot(test.index, test.DE, label='DE')\nplt.xlim((0, len(test.index)))\nplt.title('K Means')\nplt.xlabel('Data Points')\nplt.ylabel('Distance from Centroid')\nplt.legend()\nplt.show()\n\n# Calculate the accuracy of the anomaly detection.\nfrom sklearn.metrics import accuracy_score\nscore = 100 * accuracy_score(test['anomaly'], test['Fault'])\nprint(\"Accuracy: {:.2f}%\".format(score))\nprint(\"Anomalies: {}\".format(test_anomalies['anomaly'].count()))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}